# Hadoop和Spark部分

知道创宇笔试题 --  刘炜-电子科技大学-15882082269  2017-04-28

***

## 解释Spark中RDD的概念，什么是Transform和action？

### RDD

RDD(Resilient Distributed Datasets)，弹性分布式数据集，是一个容错的、并行的数据结构，可以让用户显式地将数据存储到磁盘和内存中，并能控制数据的分区。同时，RDD还提供了一组丰富的操作来操作这些数据。在这些操作中，诸如map、flatMap、filter等转换操作实现了monad模式，很好地契合了Scala的集合操作。除此之外，RDD还提供了诸如join、groupBy、reduceByKey等更为方便的操作(注意:reduceByKey是action，而非transformation)，以支持常见的数据运算。

RDD作为数据结构，本质上是一个只读的分区记录集合。一个RDD可以包含多个分区，每个分区就是一个dataset片段。RDD可以相互依赖。如果RDD的每个分区最多只能被一个Child RDD的一个分区使用，则称之为narrow dependency；若多个Child RDD分区都可以依赖，则称之为wide dependency。不同的操作依据其特性，可能会产生不同的依赖。例如map操作会产生narrow dependency，而join操作则产生wide dependency。

RDD本质上是一个内存数据集，在访问RDD时，指针只会指向与操作相关的部分。例如存在一个面向列的数据结构，其中一个实现为Int的数组，另一个实现为Float的数组。如果只需要访问Int字段，RDD的指针可以只访问Int数组，避免了对整个数据结构的扫描。

RDD天生是支持容错的。首先，它自身是一个不变的(immutable)数据集，其次，它能够记住构建它的操作图（Graph of 
Operation），因此当执行任务的Worker失败时，完全可以通过操作图获得之前执行的操作，进行重新计算。由于无需采用replication方式支持容错，很好地降低了跨网络的数据传输成本。

RDD将操作分为两类：transformation与action。无论执行了多少次transformation操作，RDD都不会真正执行运算，只有当action操作被执行时，运算才会触发。而在RDD的内部实现机制中，底层接口则是基于迭代器的，从而使得数据访问变得更高效，也避免了大量中间结果对内存的消耗。

RDD是spark的核心，也是整个spark的架构基础。有如下特性：

- 不变的数据结构存储。（RDD不可变，每次操作变成新的RDD，构成计算链条，不会产生中间计算结果）
- 支持跨集群的分布式数据结构。
- 根据数据记录的key对结构进行分区。
- 提供了粗粒度的操作，且这些操作都支持分区。
- 数据存储在内存中，从而提供了低延迟性。

### transform

transformation，顾名思义，转换，属于懒惰计算或延迟计算，当进行RDD转换时，并没有立即进行转换，仅仅是记住逻辑操作（中间过程），当进行action时才真正进行计算。 

transformation 算子操作分为两种：

- value数据类型

  - 输入分区与输出分区一对一型

    map算子

    flatMap算子

    mapPartitions算子

    glom算子:将每个分区形成一个数组

  - 输入分区与输出分区多对一型

    union算子

    cartesian算子

  - 输入分区与输出分区多对多型

    grouBy算子

  - 输出分区为输入分区子集型

    filter算子

    distinct算子

    subtract算子

    sample算子

    takeSample算子


  - Cache型

    cache算子: cache 将 RDD 元素从磁盘缓存到内存。 相当于 persist(MEMORY_ONLY) 函数的功能。

    persist算子:对RDD进行缓存操作。

- key-value数据类型

  - 输入分区与输出分区一对一

    mapValues算子

  - 对单个RDD或两个RDD聚集

    - 单个RDD聚集

      combineByKey算子
      reduceByKey算子
      partitionBy算子

    - 两个RDD聚集

      Cogroup算子

  - 连接

    join算子
    leftOutJoin和 rightOutJoin算子

    ​

### action

action: 触发SparkContext提交job作业，触发转换算子的计算。

action算子：

- 无输出

  foreach算子

- HDFS

  saveAsTextFile算子
  saveAsObjectFile算子

- Scala集合和数据类型

  collect算子
  collectAsMap算子
  reduceByKeyLocally算子
  lookup算子
  count算子
  top算子
  reduce算子
  fold算子
  aggregate算子



## 解释Spark中共享变量的使用方式？

在spark提供两种有限的共享变量：广播变量（broadCast variable）和累加器（accumulator）。

### 广播变量（broadcast variable）

广播变量允许程序员将一个**只读的变量**缓存在每台机器上，而不用在任务之间传递变量。广播变量可被用于有效地给每个节点一个大输入数据集的副本。

一个Executor只需要在第一个Tash启动时，获得一份Broadcast数据，之后的Task都从本节点的BlockManager获取相关数据。

```java
SparkConf sparkConf = new SparkConf().setAppName("ShareTest");
JavaSparkContext jsc=new JavaSparkContext(sparkConf);
//创建
Broadcast<int[]> broadcast=jsc.broadcast(new int[]{1,2,3});
//使用
broadcast.value();
```

```scala
//scala例子
object Test1 {
  private val conf = new SparkConf().setAppName("Test1")
    .setMaster("local");
  val sc = new SparkContext(conf)
  def main(args: Array[String]): Unit = {
    val bc = sc.broadcast(Array(1,2))
    val value = bc.value
    for(x<-value){
      println(x)
    }
  }
}
```



创建广播变量后，在集群上执行的任何函数都应该使用该广播变量而不是原来的值v，以使得原来的值v不会被传递到各节点多于一次。

为了保证所有节点读取相同的广播变量，对象v在被广播后不应该再修改。

### 累加器（accumulator）

提供了将工作节点中的值聚合到驱动器程序中的简单语法。累加器的一个常见用途是在调试时对作业的执行过程中事件进行计数。

累加器是仅仅被相关操作累加的变量，因此可以在并行中被有效地支持。它可以被用来实现计数器和总和。Spark原生地只支持数字类型的累加器，编程者可以添加新类型的支持。如果创建累加器时指定了名字，可以在Spark的UI界面看到。这有利于理解每个执行阶段的进程。（对于Python还不支持）

累加器通过对一个初始化了的变量v调用SparkContext.accumulator(v)来创建。在集群上运行的任务可以通过add或者"+="方法在累加器上进行累加操作。但是，它们不能读取它的值。只有驱动程序能够读取它的值，通过累加器的value方法。

```java
Accumulator<Integer> ac=jsc.accumulator(0);
JavaRDD<String> stringJavaRDD = jsc.textFile("lw.txt");
//扁平化依赖，可以将一行映射为多行
JavaRDD<String> javaRDD = stringJavaRDD.flatMap(new FlatMapFunction<String, String>() {
 	@Override
  	public Iterable<String> call(String s) throws Exception {
  	//RDD运算，分配在多个worker，不能访问累加器的值，只能对其进行增加操作。
  	ac.add(1);
  	return Arrays.asList(s.split(","));
  }
});
//action操作
javaRDD.count();
//在驱动器中可以访问累加器的值。
ac.value();
```

```scala
val arr=sc.parallelize(Array(("A",1),("B",2),("C",3)))
arr.flatMap(x=>(x._1+x._2)).foreach(println)
/**
A
1
B
2
C
3
*/
arr.map(x=>(x._1+x._2)).foreach(println)
/**
A1
B2
C3
*/
```



在action操作中，才会触发累加器，即对累加器进行真正的运算。即在flatMap为转换操作，因为transform的lazy计算，只有进行action操作才会真正执行计算，即才累加器会被触发。这里的action操作为count()。

Spark原生支持了数字类型的的累加器如：Int、Double、Long、Float等；此外Spark还支持自定义累加器用户可以通过继承AccumulableParam特征来实现自定义的累加器此外Spark还提供了accumulableCollection()累加集合；创建累加器时可以使用名字也可以不是用名字，当使用了名字时在Spark UI中可看到当中程序中定义的累加器， 广播变量存储级别为MEMORY_AND_DISK。

```java
//实现AccumulatorParam
    class MyLongAccumulator implements AccumulatorParam<Long>{

        //执行顺序为zero->addAccumulator->...->addAccumulator->addInPlace

        @Override
        public Long addAccumulator(Long aLong, Long t1) {
            return aLong+t1;
        }

        //addInPlace第一个参数为init，即累计的起始值,SparkContent.accumulator(init)。
        //这里的init是累计的起始值，不是zero中的初始值。
        //累计的起初值可以任意指定，可以不等于zero中的值。
        //第二个参数为之前累加后的数
        @Override
        public Long addInPlace(Long aLong, Long r1) {
            return aLong+r1;
        }

        @Override
        public Long zero(Long aLong) {
            //返回初始值
            return 0L;
        }

    }
```

使用：

```java
Accumulator<Integer> ac=jsc.accumulator(0L,new MyLongAccumulator());
JavaRDD<String> stringJavaRDD = jsc.textFile("lw.txt");
//扁平化依赖，可以将一行映射为多行
JavaRDD<String> javaRDD = stringJavaRDD.flatMap(new FlatMapFunction<String, String>() {
 	@Override
  	public Iterable<String> call(String s) throws Exception {
  	//RDD运算，分配在多个worker，不能访问累加器的值，只能对其进行增加操作。
  	ac.add(1L);
  	return Arrays.asList(s.split(","));
  }
});
//action操作
javaRDD.count();
//在驱动器中可以访问累加器的值。
ac.value();
```



## 解释什么是DataFrame，udf是什么，怎么使用，给出一个例子

### DataFrame

DataFrame是一个实组织成有命名的列的数据集。它在概念上等同于关系数据库中的表或R/Python的数据框架，但更加优化。DataFrames可以从各种各样的源构建，例如：结构化数据文件，Hive中的表，外部数据库或现有RDD。

DataFrame可以将原有的RDD转化为易用的数据集，计算性能也更快。

### udf

udf: user defined function, 即用户定义函数，udf为用户提供了一种更高效的方式来创建函数。即在进spark sql注册定义自己所要使用的函数，使之可以在sql语句中方便调用。

```java
JavaRDD<String> stringJavaRDD = jsc.textFile("lw.txt");
JavaRDD<Row> rowRdd = stringJavaRDD.map(new Function<String, Row>() {
  @Override
  public Row call(String s) throws Exception {
    String[] sp = s.split(",");
    return RowFactory.create(sp[0],sp[1],sp[2]);
    //return RowFactory.create((Object[]) sp);
  }
});
List<StructField> structFields=new ArrayList<StructField>();
//第三个参数nullable，表示是否可以为空,这里true表示可以为空
structFields.add(DataTypes.createStructField("colName1",DataTypes.StringType,true));
structFields.add(DataTypes.createStructField("colName2",DataTypes.StringType,true));
structFields.add(DataTypes.createStructField("colName3",DataTypes.StringType,true));
//构建StructType，用于DataFrame数据列的描述
StructType structType= DataTypes.createStructType(structFields);
SQLContext sqlContext = new SQLContext(jsc);
//构造DataFrame
DataFrame dataFrame = sqlContext.createDataFrame(rowRdd,structType);
//注册为临时表
dataFrame.registerTempTable("myDatas");
//UDF,在SQLConstext中进行注册，就可以在SQL中使用
//当然还可以用直接用udf注册外部函数，这样在DataFrame中可以直接调用函数。
sqlContext.udf().register("strLen", new UDF1<String, Integer>() {
  @Override
  public Integer call(String s) throws Exception {
    return s.length();
  }
}, DataTypes.IntegerType);
//进行数据查询
DataFrame sqlRe = sqlContext.sql("select strLen(colName1) from myDatas");
//对结果进行应用和处理
Row[] rows = sqlRe.collect();
for(Row r:rows){
  System.out.println(r.getInt(0));
}
```



## 解释SparkContext、SQLContext、HiveContext的区别

### SparkContext

sparkContext为Spark的主要入口点，是客户端的核心，用于连接Spark集群、创建RDD、累加器（accumlator）、广播变量（broadcast variables）。

### SqlContext

spark处理结构化数据的入口，允许创建DataFrame以及sql查询。

### HiveContext

spark sql执行引擎，集成hive数据，读取在classpath的 hive-site.xml配置文件配置Hive。

HiveContext 继承自SqlContext，用于需求hive数据仓库的使用。



## 如何将Spark输出结构的多个partition文件合并为一个，有几种方法

据我所知有三种方法

### 第一：rdd.coalese(1)

第一种为rdd.coalese(1)合并分区。

### 第二：dd.repartition(1)

第二种为rdd.repartition(1)重新分配分区。

### 第三：collect后写文件

第三种为通过收集分区信息然后将其写到一个文件中。

```java
//方法1：合并分区,spark写文件
stringJavaRDD.coalesce(1).saveAsTextFile("lwtest");//将会新建lwtest目录，并把数据保存为part-00000文件中
//方法2：合并分区，spark写文件
stringJavaRDD.repartition(1).saveAsTextFile("lwtest");
//方法3：先收集然后得用hadoop FileSystem写文件
Configuration hadoopConf=new Configuration();
FileSystem fs=FileSystem.get(hadoopConf);
FSDataOutputStream fileIn = fs.create(new Path("/lwtest/re.txt"));
BufferedWriter br = new BufferedWriter(new OutputStreamWriter(fileIn));

List<String> collect = stringJavaRDD.collect();

for(String s:collect){
  br.write(s);
}
br.close();
//上面方法3中，为了防止文件过大而收集失败，可以依次收集每个分区，写文件
int numPa=stringJavaRDD.getNumPartitions();
for(int i=0;i<numPa;++i){
  List<String>[] lists = stringJavaRDD.collectPartitions(new int[]{i});
  for(String s:lists){
    br.write(s);
  }
}
br.close();
```



## 如何从Spark生成的DataFrame变量获取一个块的数据，并新生成一个DataFrame？

### block

hdfs中的block是分布式存储的最小单元

### partition

spark中的partition是弹性分布式数据集RDD的最小单元，RDD是由分布在各个节点上的partition组成的。partition是指的spark在计算过程中，生成的数据在计算空间内最小单元，同一份数据（RDD）的partition大小不一，数量不定，是根据application里的算子和最初读入的数据分块数量决定的，所以名为弹性分布式数据集

### Rdd与DataFrame与DataSet

rdd适合处理非结构化处理，功能强大，但对结构化数据处理没有优化，所以产生了DataFrame，方便处理结构化数据，支持传统数据表。而Dataset整合了Rdd和DataFrame的优点，官方推荐使用，在spark2.0的API中都没有DataFrame的类了,统一为DataSet了。

### 获取一个块数据

通过DataFrame转为RDD，利用RDD的mapParitionsWithIndex，来找到对应分区的数据，然后生成新的RDD，并转换为新的DataFrame.

```scala
SparkConf conf = new SparkConf().setAppName("SparkDemo").setMaster("local");
JavaSparkContext jsc = new JavaSparkContext(conf);
List<String> data = Arrays.asList("one,lw,haha","two,xx,hh","three,yy,xx","four,fs,fs","five,fang,su");
JavaRDD<String> javaRDD = jsc.parallelize(data, 2);
JavaRDD<Row> rowRDD=javaRDD.map(new Function<String, Row>() {
  @Override
  public Row call(String s) throws Exception {
    String[] sp = s.split(",");
    return RowFactory.create(sp[0],sp[1],sp[2]);
  }
});
List<StructField> structFields=new ArrayList<StructField>();
//第三个参数nullable，表示是否可以为空,这里true表示可以为空
structFields.add(DataTypes.createStructField("colName1",DataTypes.StringType,true));
structFields.add(DataTypes.createStructField("colName2",DataTypes.StringType,true));
structFields.add(DataTypes.createStructField("colName3",DataTypes.StringType,true));
//构建StructType，用于DataFrame数据列的描述
StructType structType= DataTypes.createStructType(structFields);
SQLContext sqlContext = new SQLContext(jsc);
//构造DataFrame
DataFrame dataFrame = sqlContext.createDataFrame(rowRDD,structType);
Integer np=0;//比如获取分区为0的块的数据
JavaRDD<Row> rddPartition = dataFrame.rdd().toJavaRDD().mapPartitionsWithIndex(new Function2<Integer, Iterator<Row>, Iterator<Row>>() {
  @Override
  public Iterator<Row> call(Integer index, Iterator<Row> iterator) throws Exception {
    if (index == np) {
      return iterator;
    }
    return null;
    //return iterator;
  }
}, false).filter(new Function<Row, Boolean>() {
  @Override
  public Boolean call(Row row) throws Exception {
    if (row != null) {
      return true;
    }
    return false;
  }
});
DataFrame rpDF = sqlContext.createDataFrame(rddPartition,structType);
```



## 描述一下企业的大数据平台架构的要点，以及如何安全的开放大数据平台的服务，如何基于Kerberos实现用户的统一认证服务，给出详细的步骤流程

### 架构要点

可扩展性：就是如何处理更大规模的需求，随着业务的增长，原来的应用是否能够继续使用，并保持性能不会明显下降。可扩展的基础就是松耦合，平台架构的层次清晰，应用相对独立这是保证可扩展的前提。

安全性：数据安全与设备安全，大数据平台数据是核心，保证安全是平台正常运行的保障。实现平台访问认证，以及平台运行监控是关键。

资源利用率：资源分为计算资源和存储资源，将计算资源和存储资源分开可以灵活的充分利用两种资源，以降低成本。

性能、运行效率：运行效率关系到平台的工作处理能力，如何将目前的有的计算框架与服务整合进来，并针对不同的业务需求进行优化是提高效率的关键。

稳定性和高可用性：这是大数据平台正确处理业务的基础，在平台架构中要保障大数据平台能够在出现异常和错误时仍然能正常工作。

### 如何安全开放服务

安全开放服务，则需建立安全框架，完整企业级安全框架包括五个部分：

Administration: 大数据集群系统的集中式管理，设定全局一致的安全策略


Authentication: 对用户和系统的认证


Authorization：授权个人用户和组对数据的访问权限


Audit：维护数据访问的日志记录

Data Protection：数据脱敏和加密以达到保护数据的目的

基础设施需要覆盖以上五个部分，以保障安全，就可以安全开放服务。apache ranger和apache Sentry等可以提供hadoop生态圈的集中安全策略管理，并授权和审计。在认证方面普遍采用基于Kerberos的认证方案，Kerberos是唯一为Hadoop全面实施的验证技术。而knox可以给大数据系统提供一个很好的基于边缘的安全，即集群与外界唯一交互接口，可以通过knox处理来保证安全。

### Kerberos认证

Kerberos提供一种单点登录（SSO）的方法。

概念：

Princal(安全个体)：被认证的个体，有一个名字和口令

KDC(key distribution center ) : 是一个网络服务，提供ticket 和临时会话密钥

Ticket：一个记录，客户用它来向服务器证明自己的身份，包括客户标识、会话密钥、时间戳。

AS (Authentication Server)： 认证服务器

TGS(Ticket Granting Server)： 许可证服务器



在Kerberos中角色：认证服务器（AS），客户端（Client）和普通服务器（Server）， 许可证服务器（TGS）。客户端和服务器在AS的帮助下完成相互认证。

![img](http://dongxicheng.org/wp-content/uploads/2012/03/kerbores_work_process.jpg)

（1）Client将之前获得TGT和要请求的服务信息(服务名等)发送给KDC，KDC中的Ticket Granting Service将为Client和Service之间生成一个Session Key用于Service对Client的身份鉴别。然后KDC将这个Session Key和用户名，用户地址（IP），服务名，有效期, 时间戳一起包装成一个Ticket(这些信息最终用于Service对Client的身份鉴别)发送给Service， 不过Kerberos协议并没有直接将Ticket发送给Service，而是通过Client转发给Service，所以有了第二步。

（2）此时KDC将刚才的Ticket转发给Client。由于这个Ticket是要给Service的，不能让Client看到，所以KDC用协议开始前KDC与Service之间的密钥将Ticket加密后再发送给Client。同时为了让Client和Service之间共享那个密钥(KDC在第一步为它们创建的Session Key)，KDC用Client与它之间的密钥将Session Key加密随加密的Ticket一起返回给Client。

（3）为了完成Ticket的传递，Client将刚才收到的Ticket转发到Service. 由于Client不知道KDC与Service之间的密钥，所以它无法算改Ticket中的信息。同时Client将收到的Session Key解密出来，然后将自己的用户名，用户地址（IP）打包成Authenticator用Session Key加密也发送给Service。

（4）Service 收到Ticket后利用它与KDC之间的密钥将Ticket中的信息解密出来，从而获得Session Key和用户名，用户地址（IP），服务名，有效期。然后再用Session Key将Authenticator解密从而获得用户名，用户地址（IP）将其与之前Ticket中解密出来的用户名，用户地址（IP）做比较从而验证Client的身份。

（5）如果Service有返回结果，将其返回给Client。

参考：http://gost.isi.edu/publications/kerberos-neuman-tso.html

​	   http://dongxicheng.org/mapreduce/hadoop-kerberos-introduction/

​	   http://blog.csdn.net/jewes/article/details/20792021

​	   http://blog.csdn.net/wulantian/article/details/42418231



## 说明一下SQL on Hadoop体系的解决思路都有哪些，现有成熟方案的各自优缺点是什么

由于sql的易用性，所以sql被广泛使用。但在加强易用性的同时，也要保障效率.

hadoop的存储主要有HDFS，与HBASE，而HBASE本身就是一个数据库，有相应的信息检索机制。

那SQL on Hadoop主要是对HDFS进行检索的优化和对HBASE的集成

### 解决思路

1、数据信息结构化：在HDFS上要建立SQL查询，首先要对HDFS上的文件进行结构化，即建立数据结据，如索引，类型这些。为了不改变原来文件，将新建立的数据结构与原数据分开，这样与某些数据库的建立比较类似。

2、构建查询引擎：在数据结构化后，就是构建查询引擎。

   - 解析SQL语句，转为抽象语法树
   - 进行语义分析。
   - 逻辑执行，分析后的操作与相应查找，匹配算法相对应。
   - 优化
   - 实施物理执行


### 目前成熟方案

#### Hive

优点：

提供SQL查询分析数据的能力

不需要编写MapReduce复杂的代码

缺点：

无法实时访问数据、更新数据复杂、高延迟、速度慢

#### SparkSQL

支持Hive，构建在spark的基础上，比MapReduce更快。

#### 其它

主要是优化SQL语句中算法执行顺序，以及过滤条件的设置，以及编译器的优化，还有hadoop DAG的shuffle的优化等，其中

eg:

Cloudera的Impala

Hontonworks的Tez

Pivotal的Greenplum

星环科技的Inceptor

FaceBook的Presto



## 有哪些机制可以提高hdfs的稳定性，可用性

1、SecondaryNameNode机制

解决单点问题，这个结点周期性的从NameNode结点上下载磁盘镜像和日志文件，在本地将日志合并到镜像中，产生新的镜像，上传到NameNode，当NameNode结点重启时就会加载此最新的镜像文件，这个过程也叫做CheckPoint。SecondaryNameNode会周期性的做CheckPoint，但是这种机制对于单点问题来说不是很理想，因为做CheckPoint只能保存上次的元数据，那么CheckPoint之后的元数据在NameNode 失效后是会丢失的。

FaceBook提出了Avatar机制来解决NameNode的单点问题，这里多设置了一个叫做 Standby NameNode 的结点，原来的NameNode  叫做 PrimaryNameNode, 另外还有一个结点 NFS 用来存储 Primary NameNode 的日志和镜像文件。这里跟前面的解决机制不同的是 Standby NameNode 这个结点是 热备结点， 它不仅具有前面的CheckPoint的功能，还会周期性的读取 NFS结点上的 PrimaryNameNode 的日志来保持命名空间的同步，此外 DataNode 会同时向 Standby NameNode 和 PrimaryNameNode 发送心跳信息和数据块信息，这样，这两个结点的元数据信息是一致的。

2、HDFS数据块副本机制

在HDFS中一个文件可能有许多的数据块（Block）组成，每个数据块的副本的默认数量是3，其中两个放在同一个机架中，两一个放在其他机架，对于大数据存储来说，这种机制会造成数据的存储空间翻倍，因此需要一定的机制来节省磁盘空间，比如基于历史统计记录的动态副本策略，和FaceBook 的 RAID机制。

3、HDFS负载均衡

 NameNode 根据 DataNode发送的心跳信息和数据块信息来掌握 DataNode 的当前状态，HDFS有一个 balancer 工具 ， 可以由管理员启动，用来迁移DataNode 之间的数据块，当集群负载较高的时候不宜采用，因为可能会造成网络阻塞，造成客户端延迟过大。可以

4、MapReduce容错

运行JobTracker 的结点为主结点，用于调度作业和调度MAP 和 Reduce 任务到 TaskTracker 上， 同样TaskTracker也会周期性的向JobTracker 发送心跳信息，保护任务执行的状况，如果在指定时间内没有收到心跳信号，那么认为此结点已经宕机，重新分配 Map 或者 Reduce 任务到其他的结点上。

5、jobTracker 容错

在MapReduce中，JobTracker掌握了整个集群的运行信息，包括节点健康状况，资源分布情况以及所有作业的运行时信息。如果JobTracker因故障而重启，像节点情况以及资源情况可以利用心跳来构造，但是对于作业运行状态可能会丢失，意味着之前已经运行完成的任务会重新运行。因此，这里的关键是保存和恢复作业运行的信息。现有的技术分为3个级别的恢复机制。对于作业级别，他利用日志的方式记录完成的作业。对于任务级别，他利用日志方式记录完成的任务，当故障重启后，可以从日志中恢复作业的运行状态，即完成的任务无需运行。还有一种是从失败作业的第一条未处理的记录来恢复任务（设置断点，保存状态）。

6、TaskTracker容错

- 超时机制

  TaskTracker汇报心跳后，JobTracker会将他放到过期队列：trackerExpiryQueue中，并将其加入网络拓扑结构中。同时，TaskTracker每次汇报心跳，JobTracker会记录他的最近的心跳时间（TaskTrackerStatus.lastSeen）。同时，线程expireTrackerThread周期性的扫描队列trackerExpiryQueue，如果发现在某段时间（mapred.tasktracker.expiry.interval设置）内未汇报心跳，则将其从集群中移除。如果发现该任务处在运行或者等待状态或者是未完成的Task或者Reduce Task数目不为零的作业中已经完成的Map Task，则将会放入其他节点重新运行。

- 灰名单与黑名单

  如果作业在运行过程中记录每个TaskTracker使其失败的Task Attempt数目，一旦超过mapred.max.tracker.failures，对应的TaskTracker会被加入该作业的黑名单中。

  ​    JobTracker将记录每个TaskTracker被作业加入黑名单的次数#backlist。当某个TaskTracker同时满足一定条件（#backlist大小超过一定值（mapred.max.tracker.backlists）；该TaskTracker的#blacklist大小占所有的百分比超过一定值(mapred.cluster.average.blacklist.thredshold)；灰名单中的个数小于所有TaskTracker数目的50%），会被加入JobTracker的灰名单中。

7、Job/Task容错

一个作业会被分解成多个任务，当所有任务运行完成时，才算运行成功。然而在有些场景中，由于数据量很大，允许丢弃一小部分数据但不影响结果。所有HADOOP中设置两个参数mapred.max.map.failures.percent和mapred.max.reduce.failures.percent来控制允许失败的MAP任务与REDUCE任务的比例。

对于一个Task，他允许尝试运行多次，即Task Attempt。只要一个运行成功，则标记成功。同时当尝试次数超过阈值（mapred.map.max.attempts,mapred.reduce.max.attempts），则认为运行失败。



## 对机器学习、数据挖掘算法及其在互联网行业的应用，讲出至少3个完整案例，包括原理和工程实现

整个行业的应用大同小异，无非是应用场景稍有不同

原理：

数据挖掘方面，ETL数据清洗，提取信息特征

机器学习：大厂通常是是有自己的库，改进的算法，针对具体的场景优化。小厂一般是基于现有的开源库，主要做应用与算法的接口这块。

### 案例1：阿里广告

![t1](D:\工作\知道创宇\images\t1.jpg)

![t1](D:\工作\知道创宇\images\t2.jpg)

![t1](D:\工作\知道创宇\images\t3.jpg)

数据流：

![t1](D:\工作\知道创宇\images\t4.jpg)

技术平台，基于MPI的算法平台，最下面是hadoop集群：

![t1](D:\工作\知道创宇\images\t5.jpg)

参考：http://max.book118.com/html/2017/0321/96261636.shtm



### 案例2：小米金融

![t1](D:\工作\知道创宇\images\x1.jpg)

通过特征提取及机器学习描述用户画像

引入GBDT+LR，GBDT+FM等方法自动发现、组合特征

基于spark，利用现成api来组织应用

### 案例3：百度云平台广告推荐服务

![t1](D:\工作\知道创宇\images\b1.jpg)

推荐系统

![b2](D:\工作\知道创宇\images\b0.jpg)

架构：

![b2](D:\工作\知道创宇\images\b3.jpg)


# 编程题


- 实现一个spark的计算任务，统计采用TLS2.0协议的IP的geo分布，数据源地址：https://scans.io/zsearch/kwo7wngj2ifrww14-443-https-tls-full_ipv4-20160118T022902-zmap-results.csv.lz4

  见附件
