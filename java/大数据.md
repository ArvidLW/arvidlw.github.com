# Scala

## case class

- 创建 case class 和它的伴生 object
- 实现了 apply 方法让你不需要通过 new 来创建类实例
- 默认为主构造函数参数列表的所有参数前加 val
- 添加天然的 hashCode、equals 和 toString 方法。由于 == 在 Scala 中总是代表 equals，所以 case class 实例总是可比较的
- 生成一个 `copy` 方法以支持从实例 a 生成另一个实例 b，实例 b 可以指定构造函数参数与 a 一致或不一致
- 由于编译器实现了 unapply 方法，一个 case class 支持模式匹配
- 在模式匹配中，当你的 case class 没有参数的时候，你是在使用 case object 而不是一个空参数列表的 case class
- `unapply` 方法可以让你结构 case class 来提取它的字段，apply为注入方法，unapply为提取方法。用于在模式匹配中Hadoop

```scala
class Currency(val value: Double, val unit: String) {

}

object Currency{

  def apply(value: Double, unit: String): Currency = new Currency(value, unit)

  def unapply(currency: Currency): Option[(Double, String)] = {
    if (currency == null){
      None
    }
    else{
      Some(currency.value, currency.unit)
    }
  }
}
//作用相同于
case class Currency(value:Double,unit:String)
def main(args: Array[String]): Unit = {

    val currency = Currency(30.2, "EUR")

    currency match {
      case Currency(amount, "USD") => println("$" + amount)
      case _ => println("No match.")
    }
  }
```



## Hdfs

# Spark

## Rdd与DataFrame的区别

1、Rdd不知道数据内部结构，而DF知道，RDD是分布式java对角的集合，而DF是分布式的Row对象。

2、提升效率，Rdd强调不变性，创建新对象，而不是修改老对象，而DataFrame是尽量重用对象

3、减少数取读取，由于DF是结构化的，所以利用Spark Sql可以根据数据段的基本统计信息可以过滤它。

4、执行优化器，比如先join后filter，执行为先filter后join的

## DataSet与DataFrame

1.DataSet可以在编译时检查类型 
2.并且是面向对象的编程接口。

## Job与Task与Stage

每个action计算会生成一个job

job会被分解为Stage和Task

Stage划分的依据就是宽依赖，产生宽依赖的算子reduceByKey、groupByKey



节点类型有：

1. master 节点： 常驻master进程，负责管理全部worker节点。
2. worker 节点： 常驻worker进程，负责管理executor 并与master节点通信。

dirvier：官方解释为： The process running the main() function of the application and creating the SparkContext。即理解为用户自己编写的应用程序

## Executor：执行器：

　　在每个WorkerNode上为某应用启动的一个进程，该进程负责运行任务，并且负责将数据存在内存或者磁盘上，每个任务都有各自独立的Executor。

　　Executor是一个执行Task的容器。它的主要职责是：

　　1、初始化程序要执行的上下文SparkEnv，解决应用程序需要运行时的jar包的依赖，加载类。

　　2、同时还有一个ExecutorBackend向cluster manager汇报当前的任务状态，这一方面有点类似hadoop的tasktracker和task。

　　总结：Executor是一个应用程序运行的监控和执行容器。Executor的数目可以在submit时，由 --num-executors (on yarn)指定.

 

## Job

　　包含很多task的并行计算，可以认为**是Spark RDD 里面的action**,每个action的计算会生成一个job。

　　用户提交的Job会提交给DAGScheduler，Job会被分解成Stage和Task。

 

## Stage

　　**一个Job会被拆分为多组Task，每组任务被称为一个Stage就像Map Stage， Reduce Stage**。

　　Stage的划分在RDD的论文中有详细的介绍，简单的说是以shuffle和result这两种类型来划分。在Spark中有两类task，一类是shuffleMapTask，一类是resultTask，第一类task的输出是shuffle所需数据，第二类task的输出是result，stage的划分也以此为依据，shuffle之前的所有变换是一个stage，shuffle之后的操作是另一个stage。比如 rdd.parallize(1 to 10).foreach(println) 这个操作没有shuffle，直接就输出了，那么只有它的task是resultTask，stage也只有一个；如果是rdd.map(x => (x, 1)).reduceByKey(_ + _).foreach(println), 这个job因为有reduce，所以有一个shuffle过程，那么reduceByKey之前的是一个stage，执行shuffleMapTask，输出shuffle所需的数据，reduceByKey到最后是一个stage，直接就输出结果了。如果job中有多次shuffle，那么每个shuffle之前都是一个stage。

## Task

　　即 stage 下的一个任务执行单元，一般来说，一个 rdd 有多少个 partition，就会有多少个 task，因为每一个 task 只是处理一个 partition 上的数据.

　　每个executor执行的task的数目， 可以由submit时，--num-executors(on yarn) 来指定。

## inode

inode包含文件的元信息，具体来说有以下内容：

　　* 文件的字节数

　　* 文件拥有者的User ID

　　* 文件的Group ID

　　* 文件的读、写、执行权限

　　* 文件的时间戳，共有三个：ctime指inode上一次变动的时间，mtime指文件内容上一次变动的时间，atime指文件上一次打开的时间。

　　* 链接数，即有多少文件名指向这个inode

　　* 文件数据block的位置

 

可以用stat命令，查看某个文件的inode信息：

stat example.txt

总之，除了文件名以外的所有文件信息，都存在inode之中。至于为什么没有文件名，下文会有详细解释。

## Mllib

## Spark Streaming

# Hbase

# Kafka

# Flume



```
一面 

	 先给了一个A4纸，上面写满了题，然后就做题。做完题就自我介绍，说项目，问项目里的东西（自己一定要对项目里用的东西很熟，还要明白底层原理，我用了redis，面试官就问了redis怎么实现），然后就问java基础，面了40分钟吧，然后就让我在外面等，过了一会，二面面试官来了。 

	 试卷题目，只记得部分 

	 一、简答题 

	 1.浏览器访问一个网址的时候都有哪些过程（还要了解DNS查找的过程） 

	 2.tcp三次握手，四次分手 

	 3.线程池 

	 4.你了解的设计模式（面试官说什么单例模式就不用说了） 

	 二、编程题 

	 1.二分查找 2.树的中序遍历 

	 三、智力题 

	 给你一个5L和3L桶，水无限多，怎么到出4L 

	二面 

	 面试官问我觉得一面面的怎么样，我。。。。然后就看我做的试卷，我竟然连二分查找都写错，面试官说不对，然后我改了改就好了，然后面试官就问还有什么可以优化的地方吗？我只说出来一个。。然后又是自我介绍，说项目，问项目，问基础。差不多也是40分钟。 

	三面 

	 过了几天，收到3面邮，应该是部门boss，也是自我介绍，然后出了两个题1）一个n*n的矩阵，按副对角线打印2）4个瓶盖换1瓶酒，要和150瓶酒，他自己最少多少瓶？然后问了职业规划。感觉有1个小时。 

	Hr面 

	 北京的电面，就随便聊聊。
	 
1.网络会不会，tcp和udp区别，怎么用udp实现tcp
2.数据库会不会，说说索引怎么实现
3.一个n*n的矩阵，按副对角线打印4.4个瓶盖换1瓶酒，要喝150瓶酒，他自己最少买多少瓶？
5.职业规划？
6.如果美团和阿里同事给你offer，你去哪个
记得的就这么多了
```

# 